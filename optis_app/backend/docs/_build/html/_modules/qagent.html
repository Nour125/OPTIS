<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>qagent &#8212; OPTIS 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for qagent</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;The initial Q-learning algorithm we tried</span>
<span class="sd">Note: doesn&#39;t work on the current environment</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">environment</span> 


<div class="viewcode-block" id="q_learning"><a class="viewcode-back" href="../qagent.html#qagent.q_learning">[docs]</a><span class="k">def</span> <span class="nf">q_learning</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">activities</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Q-learning Algorithm</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    space : list of list of int</span>
<span class="sd">        contains the numbers of resources and indicates how the trace of a case looks like</span>
<span class="sd">    activities : int</span>
<span class="sd">        the number of allowed activities in the process</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    numpy.NDArray of int64</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Define the business process environment</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">BusinessProcessEnv</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">activities</span><span class="p">)</span>

    <span class="c1"># Define the Q-table</span>
    <span class="n">num_states</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">process_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">[</span><span class="s1">&#39;process&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">nvec</span> 
    <span class="c1"># case_space = env.observation_space[&#39;case&#39;].nvec </span>
    <span class="n">event_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">[</span><span class="s1">&#39;event&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">n</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">process_space</span><span class="p">:</span> <span class="n">num_states</span> <span class="o">*=</span> <span class="n">i</span>
    <span class="c1"># for i in case_space: num_states *= (i+1)</span>
    <span class="n">num_states</span> <span class="o">*=</span> <span class="n">event_space</span> 
    
    <span class="c1"># num_states = pow(2,14)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    process_space = env.observation_space[&#39;process&#39;] </span>
<span class="sd">    case_space = env.observation_space[&#39;case&#39;] </span>
<span class="sd">    event_space = env.observation_space[&#39;event&#39;] </span>

<span class="sd">    state_shape = []</span>
<span class="sd">    for i in process_space: state_shape.append(i.n + 1)</span>
<span class="sd">    for j in case_space: state_shape.append(j.n + 1)</span>
<span class="sd">    state_shape.append(event_space.n)</span>
<span class="sd">    state_shape = tuple(state_shape)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>


    <span class="c1"># Q = np.zeros(state_shape + (num_actions,), dtype=np.int8)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

    <span class="c1"># Set the hyperparameters</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>   <span class="c1"># learning rate</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># discount factor</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># exploration rate</span>

    <span class="n">mean_time</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Train the agent using Q-learning</span>
    <span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">flatten_observation_to_int</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">process</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">now</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># Choose an action based on the epsilon-greedy policy</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            
            
            <span class="c1"># Execute the action and observe the next state and reward</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

            <span class="c1"># Update the Q-value for the current state-action pair</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">])</span>
            <span class="c1">#Q[state][action] = (1-alpha)*Q[state][action] + alpha*reward </span>
            
            <span class="c1"># Transition to the next state</span>
            <span class="n">old_state</span> <span class="o">=</span> <span class="n">state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            

        <span class="n">time</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">process</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">now</span> <span class="o">-</span> <span class="n">start</span> 
        <span class="n">mean_time</span> <span class="o">+=</span> <span class="n">time</span>
        <span class="n">mean_reward</span> <span class="o">+=</span> <span class="n">reward</span>


        <span class="k">if</span> <span class="p">(</span><span class="n">episode</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">19</span><span class="p">):</span>
            <span class="n">mean_reward</span> <span class="o">/=</span> <span class="mi">20</span>
            <span class="n">mean_time</span> <span class="o">/=</span> <span class="mi">20</span> 
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="o">-</span><span class="mi">19</span><span class="si">}</span><span class="s2"> to episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: mean time = </span><span class="si">{</span><span class="n">mean_time</span><span class="si">}</span><span class="s2">, mean reward: </span><span class="si">{</span><span class="n">mean_reward</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">episode</span> <span class="o">==</span> <span class="mi">19</span><span class="p">:</span>
            <span class="n">start_reward</span> <span class="o">=</span> <span class="n">mean_reward</span>

        <span class="c1"># print(f&quot;Episode {episode}: time = {time}, reward = {reward}&quot;)</span>
        
        <span class="k">if</span> <span class="n">episode</span> <span class="o">==</span> <span class="mi">999</span><span class="p">:</span>
            <span class="n">end_reward</span> <span class="o">=</span> <span class="n">mean_reward</span>
            <span class="n">improvement</span> <span class="o">=</span> <span class="n">end_reward</span> <span class="o">-</span> <span class="n">start_reward</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reward improved by </span><span class="si">{</span><span class="n">improvement</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Q</span></div>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">OPTIS</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Backend</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, OPTIS Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
    </div>

    

    
  </body>
</html>